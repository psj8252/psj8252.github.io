---
layout: post
title: "Decision Transformer: Reinforcement Learning via Sequence Modeling ìš”ì•½"
comments: true
excerpt: "Decision Transformer: Reinforcement Learning via Sequence Modeling ë…¼ë¬¸ì„ ìš”ì•½í•´ë³´ì•˜ìŠµë‹ˆë‹¤."
tag:
- Reinforce Learning
---

<link rel="stylesheet" href="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.css" integrity="sha384-AfEj0r4/OFrOo5t7NnNe46zW/tFgW6x/bCJG8FqQCEo3+Aro6EYUG4+cU+KJWu/X" crossorigin="anonymous">

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/katex.min.js" integrity="sha384-g7c+Jr9ZivxKLnZTDUhnkOnsh30B4H0rpLUpJ4jAIKs4fnJI+sEnkvrMWph2EDg4" crossorigin="anonymous"></script>

<script defer src="https://cdn.jsdelivr.net/npm/katex@0.12.0/dist/contrib/auto-render.min.js" integrity="sha384-mll67QQFJfxn0IYznZYonOWZ644AWYC+Pt2cHqMaRhXVrursRwvLnLaebdGIlYNa" crossorigin="anonymous" onload="renderMathInElement(document.body);"></script>

- [Decision Transformer: Reinforcement Learning via Sequence Modeling](https://arxiv.org/abs/2106.01345v1)
- [ì°¸ê³ ìë£Œ](https://bellman.tistory.com/m/11)

# Abstract

---

- ì €ìë“¤ì€ Decision Transformerë¥¼ ì œì•ˆí•´ ê°•í™”í•™ìŠµì„ sequence modeling ë¬¸ì œë¡œ ì¶”ìƒí™”ì‹œì¼œ ë‹¨ìˆœí•˜ê³  í™•ì¥ì„±ì´ ì¢‹ì€ Transformer êµ¬ì¡°ì˜ ì´ì ì„ ì‚´ë¦´ ìˆ˜ ìˆê²Œ í•˜ì˜€ë‹¤.
- ë‹¨ìˆœí•¨ì—ë„ ë¶ˆêµ¬í•˜ê³  Decision TransformerëŠ” Atari, OpenAI Gym, Key-to-Door taskì˜ offline RLì—ì„œ SOTAë¥¼ ë„˜ì–´ì„œëŠ” ì„±ëŠ¥ì„ ë³´ì˜€ë‹¤.

![../assets/post_files/2021-06-29-decision-transformer/Untitled.png](../assets/post_files/2021-06-29-decision-transformer/Untitled.png)

# Introduction

---

- ì €ìë“¤ì€ Transformerë¥¼ ì „í†µ RLì˜ Componentë¡œ ì‚¬ìš©í•˜ëŠ” ì´ì „ ì—°êµ¬ë“¤ê³¼ëŠ” ë‹¤ë¥´ê²Œ Generative Trajectory modelingì´ ì „í†µì ì¸ RLì•Œê³ ë¦¬ì¦˜ì„ ëŒ€ì²´í•  ìˆ˜ ìˆëŠ”ì§€ë¥¼ ì—°êµ¬í–ˆë‹¤.
- Transformerë¥¼ ë¯¸ë¦¬ ìˆ˜ì§‘í•œ experienceë¥¼ ì´ìš©í•´ Sequence Modeling Objectiveë¥¼ ì‚¬ìš©í•´ í•™ìŠµí•˜ë„ë¡ í–ˆë‹¤.
- ì´ëŠ” ë¯¸ë˜ì˜ credit assignmentë¥¼ ì˜ˆì¸¡í•˜ê¸° ìœ„í•œ Bootstrappingì˜ í•„ìš”ì„±ì„ ì—†ì•  RLì—ì„œ deadly triad ì¤‘ í•˜ë‚˜ë¥¼ í”¼í•˜ê²Œ í•´ì¤€ë‹¤.

ğŸ’¡ [Bootstrapping?](https://velog.io/@kjb0531/%EA%B0%95%ED%99%94%ED%95%99%EC%8A%B5%EC%97%90%EC%84%9C-Bootstrapping%EC%9D%98-%EC%9D%98%EB%AF%B8)
ê°•í™”í•™ìŠµì—ì„œ ë¶€íŠ¸ìŠ¤íŠ¸ë˜í•‘ì´ë€, ì˜ˆì¸¡ê°’ì„ ì´ìš©í•´ ë˜ë‹¤ë¥¸ ê°’ì„ ì—ì¸¡í•˜ëŠ”ê²ƒì„ ë§í•œë‹¤.

- ì´ ë°©ì‹ì€ future rewardì— ëŒ€í•œ ì°¨ê°ë„ í•„ìš”í•˜ì§€ ì•Šë‹¤.
- íŠ¸ëœìŠ¤í¬ë¨¸ëŠ” self-attentionì„ í†µí•´ credit assignmentë¥¼ ì§ì ‘ì ìœ¼ë¡œ ìˆ˜í–‰í•  ìˆ˜ ìˆë‹¤.
- ì €ìë“¤ì€ offline RLì—ì„œ ê°€ì„¤ì„ í…ŒìŠ¤íŠ¸í–ˆë‹¤.

ğŸ’¡ [Offline RL?](https://talkingaboutme.tistory.com/entry/RL-Offline-Reinforcement-Learning)
ì‹¤ì œë¡œ ë§¤ë²ˆ ì˜ˆì¸¡ì„ ìˆ˜í–‰í•˜ë©° ë‹¤ìŒ ìƒíƒœë¥¼ íƒìƒ‰í•˜ëŠ” onlineê³¼ ë‹¬ë¦¬ ê³ ì •ëœ ë°ì´í„°ì…‹ìœ¼ë¡œ ìµœì ì˜ Policyë¥¼ ì°¾ëŠ”ë‹¤.

- ì €ìë“¤ì€ Autoregressiveí•˜ê²Œ Trajectoryë¥¼ ìƒì„±í•˜ëŠ” GPT ì•„í‚¤í…ì³ë¡œ Decision Transformerë¥¼ Atari, OpenAI Gym, Key-to-Door í™˜ê²½ì—ì„œ offline RL ë²¤ì¹˜ë§ˆí¬ë¡œ ê²€ì¦í–ˆë‹¤.

    ê·¸ ê²°ê³¼ dynamic programming ì—†ì´ SOTA model-free offline RL ì•Œê³ ë¦¬ì¦˜ì„ ë„˜ì–´ì„°ë‹¤.

ğŸ’¡ [Dynamic Programming?](https://brunch.co.kr/@chris-song/81)
í¬ê³  ë³µì¡í•œ ë¬¸ì œëŠ” ì‘ì€ ë¬¸ì œë“¤ë¡œ ìª¼ê°œì„œ í‘¼ë‹¤ëŠ” ì»¨ì…‰

# Preliminaries

---

## Offline reinforcement learning

ì €ìë“¤ì€ $$(\mathcal{S}, \mathcal{A}, P, \mathcal{R})$$ë¡œ ì„¤ëª…í•  ìˆ˜ ìˆëŠ” Markov decision process(MDP)ì—ì„œì˜ í•™ìŠµìœ¼ë¡œ ëª¨ë¸ë§í–ˆë‹¤. 

MDP íŠœí”Œì€ state $$s \in \mathcal{S}$$, 
action $$a \in \mathcal{A}$$, 
transition dynamics $$P(s'|s,a)$$, 
reward í•¨ìˆ˜ $$r = \mathcal{R}(s,a)$$ë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤.

timestep $$t$$ì—ì„œì˜ state, action, rewardë¥¼ $$s_t, a_t, r_t=\mathcal{R}(s_t, a_t)$$ë¡œ ì‚¬ìš©í•œë‹¤.

trajectoryëŠ” states, actions, rewardsì˜ sequenceë¡œ êµ¬ì„±ëœë‹¤. 

$$\tau = (s_0, a_0, r_0, s_1, a_1, r_1, \cdots, s_T, a_T, r_T)$$

time $$t$$ì—ì„œ trajectoryì˜ ë³´ìƒì€ $$R_t = \sum_{t'=t}^T r_{t'}$$ ê·¸ timestepë¶€í„° ë¯¸ë˜ì˜ ë³´ìƒì˜ í•©ì´ë‹¤.

ê°•í™”í•™ìŠµì˜ ëª©í‘œëŠ” MDPì—ì„œ ì˜ˆìƒë˜ëŠ” ë³´ìƒ $$\mathbb{E}[\sum_{t=1}^T r_t]$$ ë¥¼ ìµœëŒ€í™”í•˜ëŠ” policyë¥¼ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤.

offline RLì—ì„œëŠ” agentê°€ í™˜ê²½ì„ íƒìƒ‰í•˜ê³  ì¶”ê°€ì ì¸ í”¼ë“œë°±ì„ ë°›ì„ ìˆ˜ ìˆëŠ” ëŠ¥ë ¥ì„ ì—†ì• ê¸° ë•Œë¬¸ì— ë” ì–´ë ¤ìš´ í™˜ê²½ì´ë‹¤.

## Transformers

TransformerëŠ” sequential dataë¥¼ íš¨ê³¼ì ìœ¼ë¡œ ë‹¤ë£° ìˆ˜ ìˆëŠ” ì•„í‚¤í…ì³ì´ë‹¤. TransformerëŠ” ì—¬ëŸ¬ self-attentionë ˆì´ì–´ì™€ residual connectionìœ¼ë¡œ êµ¬ì„±ë˜ì–´ìˆë‹¤. 

ì´ êµ¬ì¡°ëŠ” Query-Key ë²¡í„° ìƒì—ì„œ stateì™€ ë³´ìƒê³¼ì˜ ê´€ê³„ë¥¼ ë‚´ì¬ì ìœ¼ë¡œ í˜•ì„±í•´ credit assignì„ ê°€ëŠ¥í•˜ê²Œ í•œë‹¤.

# Method

---

## Trajectory representation

trajectory í‘œí˜„ì—ì„œ í•µì‹¬ì€ íŠ¸ëœìŠ¤í¬ë¨¸ê°€ ì˜ë¯¸ìˆëŠ” íŒ¨í„´ì„ ë°°ìš°ê³  test timeì˜ ì¡°ê±´í•˜ì—ì„œ actionì„ ìƒì„±í•  ìˆ˜ ìˆê²Œ ë§Œë“œëŠ” ê²ƒì´ë‹¤. ìš°ë¦¬ëŠ” ëª¨ë¸ì´ ë¯¸ë˜ì˜ ê¸°ëŒ€ë˜ëŠ” ë³´ìƒ(returns-to-go)ì— ê¸°ë°˜í•´ actionì„ ìƒì„±í•˜ê¸¸ ì›í•˜ê¸° ë•Œë¬¸ì— ì´ëŠ” ë‹¨ìˆœí•œ ë¬¸ì œê°€ ì•„ë‹ˆë‹¤. ê²°ê³¼ì ìœ¼ë¡œ ì €ìë“¤ì€ ëª¨ë¸ì—ê²Œ ë³´ìƒì„ ì§ì ‘ ì£¼ì§€ ì•Šê³  ëª¨ë¸ì—ê²Œ ë¯¸ë˜ì˜ ë³´ìƒ $$\widehat{\mathcal{R}}_t = \sum_{t'=t}^T r_{t'}$$ì„ ì£¼ì—ˆë‹¤.

autoregressiveí•œ í•™ìŠµê³¼ ìƒì„±ì— ì‰¬ìš´ ì•„ë˜ì™€ ê°™ì€ trajectoryë¡œ í‘œí˜„ëœë‹¤.

$$\tau = (\widehat{R}_1, s_1, a_1, \widehat{R}_2, s_2, a_2, \cdots, \widehat{R}_T, s_T, a_T)$$

## Architecture

ì €ìë“¤ì€ last $$K$$ timestepì˜ ì •ë³´ë¥¼ ëª¨ë¸ ì…ë ¥ìœ¼ë¡œ ì‚¬ìš©í–ˆë‹¤. ê° ìŠ¤í…ë§ˆë‹¤ return-to-go, state, actionì´ ìˆìœ¼ë¯€ë¡œ $$3K$$ í† í°ì„ ì‚¬ìš©í•œë‹¤. tokenì„ Embeddingìœ¼ë¡œ ë§Œë“¤ê¸° ìœ„í•´ Embedding layerë¥¼ ê±°ì¹˜ê³  layer normalizationì„ ì ìš©í•œë‹¤. (ì‹œê°ì  ì…ë ¥ì˜ ê²½ìš°ì—ëŠ” ëŒ€ì‹  Convolutionì„ ì‚¬ìš©í•œë‹¤.) ì›ë˜ GPTì™€ ë‹¬ë¦¬ í•˜ë‚˜ì˜ Positional Encodingì€ ì„¸ í† í°ì— ì ìš©ëœë‹¤ëŠ” ì°¨ì´ê°€ ìˆë‹¤. 

## Training

length $$K$$ì˜ sequenceë¥¼ ë°ì´í„°ì…‹ìœ¼ë¡œë¶€í„° ìƒ˜í”Œë§í•œë‹¤. ëª¨ë¸ì€ ì…ë ¥ í† í° $$s_t$$ë¡œë¶€í„° $$a_t$$ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ í•™ìŠµëœë‹¤. (ì´ì‚°ì ì¸ actionì´ë©´ cross entropy, ì—°ì†ì ì¸ actionì´ë©´ MSE loss) ê° timestepì˜ lossë¥¼ í‰ê· ë‚´ì„œ ì‚¬ìš©í•œë‹¤. stateë‚˜ returns-to-goë¥¼ ì˜ˆì¸¡í•˜ëŠ” ê²ƒì€ ì„±ëŠ¥ì— ë³€í™”ê°€ ì—†ì—ˆë‹¤.

![../assets/post_files/2021-06-29-decision-transformer/Untitled%201.png](../assets/post_files/2021-06-29-decision-transformer/Untitled%201.png)

# Evaluation on Offline RL Benchmarks

---

Decision Transformerì˜ ì„±ëŠ¥ì„ dedicated offline RLê³¼ imitation í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ê³¼ ë¹„êµí•˜ì˜€ë‹¤. íŠ¹íˆ Decision Transformerë„ model-free ë°©ì‹ì´ê¸° ë•Œë¬¸ì— TD-learning ê¸°ë°˜ì˜ model-free offline RL ì•Œê³ ë¦¬ì¦˜ê³¼ ì£¼ë¡œ ë¹„êµí–ˆë‹¤. 

ğŸ’¡ [model-free?](https://www.secmem.org/blog/2019/12/15/RL-key-concepts#model-based-model-free)
í™˜ê²½ì— ëŒ€í•œ ì˜ˆì¸¡ì„ ì‚¬ìš©í•˜ëŠ” ë°©ì‹ì´ model-based, ì‚¬ìš©í•˜ì§€ ì•ŠëŠ” ë°©ì‹ì´ model-free

ë˜í•œ likelihood ê¸°ë°˜ì˜ policy learning ê³µì‹ì´ ìœ ì‚¬í•˜ê¸° ë•Œë¬¸ì— behavior cloning ì¢…ë¥˜ì™€ë„ ë¹„êµí–ˆë‹¤.

![../assets/post_files/2021-06-29-decision-transformer/Untitled%202.png](../assets/post_files/2021-06-29-decision-transformer/Untitled%202.png)

ì €ìë“¤ì€ ì´ì‚°ì ì¸ actionì„ í•„ìš”í•œ Atariì™€ ì—°ì†ì ì¸ actionì´ í•„ìš”í•œ OpenAI Gymì— ëª¨ë‘ í‰ê°€í–ˆë‹¤. ì „ìëŠ” ê³ ì°¨ì›ì˜ observation ê³µê°„ì´ í•„ìš”í•˜ê³  long-term credit assignmentë¥¼ ì‚¬ìš©í•˜ë©° í›„ìëŠ” ì„¸ì„¸í•œ controlì„ ìš”í•œë‹¤. ê²°ê³¼ëŠ” ìœ„ì™€ ê°™ë‹¤.

## Atari

ì•„íƒ€ë¦¬ëŠ” ê³ ì°¨ì›ì˜ ì‹œê°ì…ë ¥ê³¼ ì§€ì—°ëœ ë³´ìƒìœ¼ë¡œ ì¸í•´ ì–´ë µë‹¤. ë„¤ ì¢…ë¥˜ì˜ ëª¨ë¸ CQL, REM, QR-DQNì„ ë„¤ ê°œì˜ ì•„íƒ€ë¦¬ Task(Breakout, Qbert, Pong, Seaquest)ì— ëŒ€í•´ ë¹„êµí–ˆë‹¤. 

![../assets/post_files/2021-06-29-decision-transformer/Untitled%203.png](../assets/post_files/2021-06-29-decision-transformer/Untitled%203.png)

ì €ìë“¤ì˜ ë°©ë²•ì€ 3ê°œì˜ ê²Œì„ì€ CQLì— ë¹„ê¸°ëŠ” ê²°ê³¼ë¥¼ ë³´ì—¬ì£¼ì—ˆê³  ëª¨ë“  ê²Œì„ì—ì„œ ë‚˜ë¨¸ì§€ 3ê°œ ëª¨ë¸ë³´ë‹¤ ì¢‹ì€ ê²°ê³¼ë¥¼ ì–»ì—ˆë‹¤.

## OpenAI Gym

OpenAI Gymì˜ HalfCheetah, Hopper, Walker, Reacher ê²Œì„ìœ¼ë¡œ CQL, BEAR, BRAC, AWRëª¨ë¸ë“¤ê³¼ ë¹„êµí–ˆë‹¤. CQLì€ model-free offline RLì—ì„œ SOTAì´ë‹¤.

ê° taskë§ˆë‹¤ ì•„ë˜ì²˜ëŸ¼ ì„¸ ì¢…ë¥˜ì˜ ë°ì´í„°ì…‹ìœ¼ë¡œ ë‚˜ëˆ ì‹¤í—˜í–ˆë‹¤.

1. Medium: "expert" policyì˜ 1/3 ì •ë„ ì ìˆ˜ì— ë„ë‹¬í•œ "medium" policyë¡œ ìƒì„±ëœ 100ë§Œê°œì˜ timestepë¥¼ ì‚¬ìš©
2. Medium-Replay: medium policyë¡œ í•™ìŠµëœ agentì˜ replay ë²„í¼ (ì €ìë“¤ì˜ í™˜ê²½ì—ì„  25k-400k timestep)
3. Medium-Expert: mediumë¡œ ìƒì„±ëœ 100ë§Œê°œì˜ timestepê³¼ expert policyë¡œ ìƒì„±ëœ 100ë§Œê°œì˜ timestepë¥¼ í•¨ê»˜ ì‚¬ìš©

ğŸ’¡ [Experience reply?](http://sanghyukchun.github.io/90/)
ì´ì „ì˜ state, action, return ë“±ì„ ë²„í¼ì— ì €ì¥í•˜ê³  ìˆë‹¤ê°€ ë‚˜ì¤‘ì— batchì— ëœë¤í•˜ê²Œ ë½‘ì•„ í•¨ê»˜ ë„£ì–´ í•™ìŠµí•˜ëŠ” ê²ƒ

![../assets/post_files/2021-06-29-decision-transformer/Untitled%204.png](../assets/post_files/2021-06-29-decision-transformer/Untitled%204.png)

Decision TransformerëŠ” ë‹¤ìˆ˜ì˜ ì‘ì—…ì—ì„œ ê°€ì¥ ë†’ì€ ì ìˆ˜ë¥¼ ì–»ì—ˆìœ¼ë©° ë‚˜ë¨¸ì§€ì— ëŒ€í•´ì„œë„ ë¹„ë“±í•œ ì ìˆ˜ë¥¼ ì–»ì—ˆë‹¤.

# Discussion

---

### Does Decision Transformer perform behavior cloning on a subset of the data?

Decision Transformerê°€ íŠ¹ì • ë³´ìƒì„ ê°€ì§„ ë°ì´í„°ì…‹ì— ëŒ€í•´ imitation learningì„ í•˜ëŠ” ê²ƒì´ì§€ ì•Œê¸° ìœ„í•´ ì‹¤í—˜í–ˆë‹¤. Percentile Behavior Cloning (%BC)ì™€ ë¹„êµí•˜ëŠ” ë°©ë²•ì„ ì‚¬ìš©í–ˆëŠ”ë° ì´ëŠ” returnì´ ë†’ì€ ìˆœì„œëŒ€ë¡œ ì •ë ¬í–ˆì„ ë•Œ ìƒìœ„ X%ì˜ trajectoryë¥¼ ì‚¬ìš©í•œ ë°ì´í„°ë¡œ Behavior Cloningì„ í•™ìŠµí•˜ëŠ” ê²ƒì´ë‹¤. X = 100% ì¸ ê²½ìš°ëŠ” ëª¨ë“  ë°ì´í„°ì…‹ì„ ì‚¬ìš©í•˜ëŠ” ê²ƒì´ë‹¤. 

![../assets/post_files/2021-06-29-decision-transformer/Untitled%205.png](../assets/post_files/2021-06-29-decision-transformer/Untitled%205.png)

ë°ì´í„°ê°€ ì¶©ë¶„í•  ë•Œ %BCëŠ” ë‹¤ë¥¸ offline RL ë°©ë²•ì„ ì´ê¸°ê±°ë‚˜ ë¹„ìŠ·í•˜ë‹¤. 

![../assets/post_files/2021-06-29-decision-transformer/Untitled%206.png](../assets/post_files/2021-06-29-decision-transformer/Untitled%206.png)

í•˜ì§€ë§Œ ì•„íƒ€ë¦¬ì²˜ëŸ¼ ë°ì´í„°ê°€ ë¶€ì¡±í•œ ê²½ìš°ì—ëŠ” %BCëŠ” ì•½í•˜ë‹¤. ì´ëŠ” ì ì€ ë°ì´í„°ì—ì„œ Decision TransformerëŠ” ëª¨ë“  trajectoryë¥¼ ì‚¬ìš©í•œ %BCë³´ë‹¤ ì„±ëŠ¥ì´ ë” ë†’ê³  ì¼ë°˜í™” ëŠ¥ë ¥ì´ ì¢‹ë‹¤ëŠ” ì ì„ ì‹œì‚¬í•œë‹¤.

ì´ ê²°ê³¼ë¥¼ ë´¤ì„ ë•Œ Decision TransformerëŠ” ë‹¨ìˆœíˆ imitation learningì„ ìˆ˜í–‰í•˜ëŠ” ê²ƒë³´ë‹¤ ë” íš¨ê³¼ì ì¼ ìˆ˜ ìˆë‹¤.

### How well does Decision Transformer model the distribution of returns?

Decision Transformerê°€ returns-to-go í† í°ì„ ì´í•´í•˜ê³  ìˆëŠ” ëŠ¥ë ¥ì„ desired target returnì„ ë³€ê²½í•˜ë©´ì„œ í‰ê°€í–ˆë‹¤. 

![../assets/post_files/2021-06-29-decision-transformer/Untitled%207.png](../assets/post_files/2021-06-29-decision-transformer/Untitled%207.png)

ìœ„ ê·¸ë¦¼ì€ ë‹¤ì–‘í•œ target returnì˜ ê°’ì— ë”°ë¼ agentê°€ íšë“í•œ ëˆ„ì  returnì„ ë³´ì—¬ì¤€ ê²ƒì´ë‹¤. ëª¨ë“  taskì—ì„œ desired target returnê³¼ ì‹¤ì œ ê´€ì¸¡ëœ returnì€ ë†’ì€ ìƒê´€ê´€ê³„ë¥¼ ë³´ì˜€ë‹¤. 

ì—¬ê¸°ì„œ ì‹¤ì œë¡œ ë°ì´í„°ì…‹ì— ì¡´ì¬í•˜ëŠ” ìµœëŒ€ì˜ returnë³´ë‹¤ ë†’ì€ returnì„ ì–»ì€ ê²½ìš°ë„ ìˆëŠ”ë° Decision Transformerì˜ ì¶”ë¡ ì  ëŠ¥ë ¥ì„ ë³´ì—¬ì¤€ë‹¤.

### What is the benefit of using a longer context length?

context lengthì˜ ì¤‘ìš”ì„±ì„ ì‹¤í—˜í•˜ê¸° ìœ„í•´ context length $$K$$ë¡œ ablationì„ ì§„í–‰í–ˆë‹¤. ì´ëŠ” frame stackingì„ ì‚¬ìš©í–ˆì„ ë•Œ ë³´í†µ ì§ì „ ìƒíƒœ($$K=1$$) ì´ ê°•í™”í•™ìŠµ ì•Œê³ ë¦¬ì¦˜ì— ì¶©ë¶„í•˜ë‹¤ê³  ì—¬ê²¨ì§€ê¸° ë•Œë¬¸ì— í¥ë¯¸ë¡œìš´ ì ì´ë‹¤. 

![../assets/post_files/2021-06-29-decision-transformer/Untitled%208.png](../assets/post_files/2021-06-29-decision-transformer/Untitled%208.png)

ê²°ê³¼ëŠ” ìƒë‹¹í•œ ì„±ëŠ¥ì°¨ì´ë¥¼ ë³´ì—¬ì£¼ì—ˆë‹¤. í•œ ê°€ì§€ ê°€ì„¤ì€ ìš°ë¦¬ê°€ policyë“¤ì˜ ë¶„í¬ë¥¼ í‘œí˜„í•  ë•Œ ì–´ë–¤ Policyê°€ actionì„ ìƒì„±í•˜ëŠ”ì§€ ì•Œê²Œí•˜ê³  í•™ìŠµì„ ë•ëŠ”ë‹¤ëŠ” ê²ƒì´ë‹¤.

### Does Decision Transformer perform effective long-term credit assignment?

ëª¨ë¸ì˜ long-term credit assignment ëŠ¥ë ¥ì„ í‰ê°€í•˜ê¸° ìœ„í•´ Key-to-Door ë¥˜ì˜ í™˜ê²½ì„ ì‚¬ìš©í–ˆë‹¤. ì´ í™˜ê²½ì€ ë‹¤ìŒ 3 í˜ì´ì¦ˆë¡œ ì§„í–‰ëœë‹¤.

1. agentëŠ” í‚¤ê°€ ìˆëŠ” ë°©ì— ë†“ì¸ë‹¤.
2. agentëŠ” ë¹ˆ ë°©ì— ë†“ì¸ë‹¤.
3. agentëŠ” ë¬¸ì´ ìˆëŠ” ë°©ì— ë†“ì¸ë‹¤. 

agentê°€ ë¬¸ì— phase 3ì—ì„œ ë¬¸ì— ë„ì°©í•˜ë©´ binary rewardë¥¼ ë°›ëŠ”ë‹¤. ë‹¨ phase 1ì—ì„œ keyë¥¼ ì§‘ì—ˆì„ ê²½ìš°ì—ë§Œ.

![../assets/post_files/2021-06-29-decision-transformer/Untitled%209.png](../assets/post_files/2021-06-29-decision-transformer/Untitled%209.png)

random trajectoryë¡œ ìƒì„±í•œ ë°ì´í„°ì…‹ì„ ì´ìš©í•´ ëª¨ë¸ë“¤ì„ í•™ìŠµì‹œì¼°ë‹¤. Decision Transformerì™€ %BCëŠ” random walkë¡œ ë§Œë“  ë°ì´í„°ì„ì—ë„ íš¨ê³¼ì ì¸ policyë¥¼ í•™ìŠµí•  ìˆ˜ ìˆì—ˆë‹¤. TD learningì€ ì œëŒ€ë¡œ í•™ìŠµí•˜ì§€ ëª»í–ˆë‹¤.

### Does Decision Transformer perform well in sparse reward settings?

TD learningì˜ ì•Œë ¤ì§„ ì•½ì ì€ ê³¨ê³ ë£¨ ì¶©ë¶„í•œ ë³´ìƒì´ ì£¼ì–´ì ¸ì•¼ ì˜ í•™ìŠµí•  ìˆ˜ ìˆë‹¤ëŠ” ì ì´ë‹¤. ê·¸ë¦¬ê³  ê·¸ê²ƒì€ ë¹„í˜„ì‹¤ì ì¸ ê°€ì •ì´ë‹¤. Decision TransformerëŠ” ê·¸ ë¶€ë¶„ì—ì„œ ë” robustí•  ìˆ˜ ìˆë‹¤. ì´ë¥¼ í‰ê°€í•˜ê¸° ìœ„í•´ Hopper taskì—ì„œ ë³´ìƒì„ ê³„ì† ì£¼ì§€ ì•Šë‹¤ê°€ ë§ˆì§€ë§‰ timestepì— ëª°ì•„ì„œ ì£¼ë„ë¡ ë°”ê¿”ì„œ ì‹¤í—˜í–ˆë‹¤.

![../assets/post_files/2021-06-29-decision-transformer/Untitled%2010.png](../assets/post_files/2021-06-29-decision-transformer/Untitled%2010.png)

TD learning ë°©ì‹ì€ ì„±ëŠ¥ì´ í¬ê²Œ í•˜ë½í–ˆì§€ë§Œ DTëŠ” robustí–ˆë‹¤. BCëŠ” í•™ìŠµ ë°©ë²• ìƒ Rewardì™€ ë¬´ê´€í•˜ë‹¤.

### Why does Decision Transformer avoid the need for value pessimism or behavior regularization?

Decision Transformerì™€ ë‹¤ë¥¸ offline RL ì•Œê³ ë¦¬ì¦˜ì˜ í° ì°¨ì´ì ì€ DTëŠ” policy regularizationì´ë‚˜ convervatismì´ í•„ìš”í•˜ì§€ ì•Šë‹¤ëŠ” ì ì´ë‹¤. 

ì €ìë“¤ì˜ ì¶”ì¸¡ì€ TD learning ê¸°ë°˜ì˜ ì•Œê³ ë¦¬ì¦˜ì€ ê·¼ì‚¬ value functionì„ ì¶”ì •í•˜ê³  value functionì„ ê°œì„ í•˜ë©´ì„œ policyë¥¼ ê°œì„ í•˜ëŠ”ë° í•™ìŠµëœ í•¨ìˆ˜ë¥¼ ìµœì í™”í•˜ëŠ” ê²ƒì€ value functionê·¼ì‚¬ì˜ ë¶€ì •í™•ì„±ì„ ì•…í™”ì‹œí‚¨ë‹¤. DTëŠ” ì§ì ‘ì ìœ¼ë¡œ í•™ìŠµëœ í•¨ìˆ˜ë¥¼ ëª©ì í•¨ìˆ˜ë¡œ ì‚¬ìš©í•˜ì§€ ì•Šê¸° ë•Œë¬¸ì— ì´ëŸ° ë¬¸ì œë¥¼ í”¼í•  ìˆ˜ ìˆëŠ” ê±° ê°™ë‹¤.

## How can Decision Transformer benefit online RL regime?

Decision TransformerëŠ” Onlineì—ì„œë„ ì˜ ë™ì‘í•  ê²ƒì´ë¼ê³  ë¯¿ëŠ”ë‹¤.

# Conclusion

---

ì €ìë“¤ì€ Decision Transformerë¥¼ ì œì•ˆí•˜ë©° language/sequence modelingê³¼ ê°•í™”í•™ìŠµì„ í•©ì¹˜ëŠ” ì•„ì´ë””ì–´ë¥¼ ëƒˆë‹¤. ì¼ë°˜ LMì—ì„œ ì¡°ê¸ˆ ë‹¬ë¼ì§„ êµ¬ì¡°ì˜ Decision Transformerë¥¼ ì‚¬ìš©í•˜ì—¬ ë§ì€ ê°•ë ¥í•œ offline RL ì•Œê³ ë¦¬ì¦˜ì„ ì´ê¸°ê±°ë‚˜ ë¹„ìŠ·í•œ ì„±ëŠ¥ì„ ë³´ì—¬ì£¼ì—ˆë‹¤. 

ìš°ë¦¬ëŠ” supervised lossë¥¼ ì‚¬ìš©í–ˆì§€ë§Œ í° ë°ì´í„°ì…‹ìœ¼ë¡œ self-supervised pretraining taskë¡œ ë” ê°œì„ ë  ìˆ˜ ìˆì„ì§€ë„ ëª¨ë¥¸ë‹¤. íŠ¹íˆ state, return, actionì— ëŒ€í•œ ë” ì •êµí•œ embeddingì„ ê³ ë ¤í•´ë³¼ ìˆ˜ ìˆë‹¤. Transformer ëª¨ë¸ì€ stateì˜ ë³€í™”ë¥¼ ëª¨ë¸ë§í•˜ëŠ”ë° ì‚¬ìš©ë  ìˆ˜ ìˆê¸° ë•Œë¬¸ì— ì ì¬ì ìœ¼ë¡œ model-based RLë¥¼ ëŒ€ì²´í•  ê°€ëŠ¥ì„±ì´ ìˆë‹¤.
